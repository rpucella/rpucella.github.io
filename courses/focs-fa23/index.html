<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
    <title>Foundations of Computer Science (Spring 2023)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="stylesheet" href="/static/main.css" type="text/css">
    <link rel="stylesheet" href="/courses/course.css" type="text/css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap" rel="stylesheet">

    <script src="/static/smooth-scroll.js"></script>

    <style>
      body {
          font-size: 120%;
      }

      section {
          border-top: 1px solid rgb(90, 0, 0);
      }
    </style>

  </head>

  <body>

    <nav>

      <div class="home">
	<a href="/"><img src="/home.png" style="height: 1.6em;"></a>
      </div>

      <ul>
	<li><a class="smooth-scroll" href="#main"><b>Foundations of Computer Science SP23</b></a></li> 
	<li><a class="smooth-scroll" href="#info">Course Info</a></li>
	<li><a class="smooth-scroll" href="#lectures">Lectures</a></li>
	<li><a class="smooth-scroll" href="#homeworks">Homeworks</a></li>
      </ul>
    </nav>


    
<main id="main">
  
      <h1>Foundations of Computer Science (Fall 2023)</h1>
    
      <p>This course explores the notion of computation. We're going to
        develop formal tools for defining what we mean by computation
        through various models, including automata and the lambda
        calculus. We will examine how some of these alternative models
        of computations correspond to different programming paradigms.
      </p>
      
      <section id="info">
        <h2>Course Information</h2>

        <ul class="plain">
          <li><b>Course number:</b> ENGR 3520</li>

          <li><b>Prerequisites:</b> Prior experience programming is required (Software Design or something equivalent) since I will not teach programming. Discrete Mathematics is no longer a formal prerequisite for FoCS, but it may make your life easier.</li>

          <li><b>Location and Time:</b>
            MAC 328 / Mon 6-8:40pm
          </li>

          <li><b>Instructor:</b> <a href="https://www.rpucella.net">Riccardo Pucella</a> (&#x72;&#x69;&#x63;&#x63;&#x61;&#x72;&#x64;&#x6F;&#x2E;&#x70;&#x75;&#x63;&#x65;&#x6C;&#x6C;&#x61;&#x40;&#x6F;&#x6C;&#x69;&#x6E;&#x2E;&#x65;&#x64;&#x75;)</li>

          <li><b>Office hours:</b> MH 353 / Mon 4-5pm (before class) / I'm available at other times over Zoom by request
          </li>

          <li><b>Textbook:</b> There is no required textbook for the course. We will be working off
            notes and online references.
          </li>

          <li><b>Recommended Books:</b> There are several excellent books covering the first part of
            the course (aka, theory of computation), but they are a bit expensive considering that
            we will be covering only a small part of their content. If you need extra assistance,
            though, any of these books would be useful, and they are on reserve in the Olin library:
            
            <ul>
              <li>Chen, <i><a href="https://www.amazon.com/Computability-Complexity-Hubie-Chen/dp/0262048620">Computability and Complexity</a></i></li>
              <li>Sipser, <i><a href="https://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/1133187811">Introduction to the Theory of Computation</a></i></li>
	      <li>Hopcroft, Motwani, Ullman, <i><a href="https://www.amazon.com/Introduction-Automata-Theory-Languages-Computation/dp/0321455363">Automata Theory, Languages, and Computation</a></i></li>
            </ul>
            
            An inexpensive and reasonable book is the following, although it uses Ruby instead of Haskell:
            <ul>
	      <li>Stuart, <i><a href="http://computationbook.com">Understanding Computation</a></i></li>
            </ul>
            It very much follows the spirit of this iteration of FoCS. Again, not required. But if you enjoy the course, you may enjoy the book.
            
          </li>

          
          <li><p><b>Programming:</b> All programming in this course will be done using the programming
            language <a href="https://www.haskell.org/">Haskell</a>. The Haskell compiler (GHC) can
            be downloaded from the web site, and supports installations on most common systems. You
            can find the installation
              instructions <a href="https://www.haskell.org/get-started/">here</a>. (Don't worry about Cabal or Stack — they will get installed by default, but we're not going to use them.) Installation steps from the link above: install GHCup (which install the required GHC, and optionally lets you install HLC which you need in order to use VSCode or related editor in a way that can do syntax highlighting); install VSCode; install the Haskell extension to VSCode.</p>

            <p>Once Haskell is installed, type <tt>ghci</tt> in the terminal to start the interactive shell. That's mostly what we'll be using. Type <tt>:quit</tt> to quit.</p>

            <p>There are a couple of reasonable books on Haskell that you might want to get your hands
            on if you prefer learning your programming language from a book instead of the web:
            
            <ul>
              <li>Hutton, <i><a href="https://www.cambridge.org/us/universitypress/subjects/computer-science/programming-languages-and-applied-logic/programming-haskell-2nd-edition">Programming in Haskell</a></i></li>
              <li>Allen, Moronuki, <i><a href="https://haskellbook.com/">Haskell Programming From First Principles</a></i></li>
            </ul>

            Useful onlines resources can be found on the <a href="https://www.haskell.org">Haskell website</a>. In particular, the <a href="https://www.haskell.org/documentation/">Documentation</a> section contains links to tutorials that may be useful. Note the introduction link to <a href="https://www.seas.upenn.edu/~cis1940/spring13/lectures.html">CIS194</a>, an introductory course on Haskell. The <a href="https://www.seas.upenn.edu/~cis1940/spring13/lectures/01-intro.html">first CIS194 lecture</a>, in particular, is all we really need to get started.</p>
          </li>
          
          <li><b>Grading:</b> The final grade is based on weekly
            homework (40%), in-class quizzes (20%), and a final project
            (40%). All work will be done individually. Late homeworks
            will be penalized (5% per 24 hours) and no homework will
            be accepted after its solution has been discussed in
            class. <b>Please do not post your homework code on a
            publicly available repository like GitHub.</b> I hate to
            do this kind of policing, but we've had some issues in the
            past, and I'd like to avoid a repeat if possible.
          </li>

          <li><b>Course Assistants:</b> Ben Morris (<tt>bmorris</tt> at the
            usual olin.edu domain) <!-- Office hours: TBD -->
          </li>

          <li>I expect all of us to follow the <b><a href="http://www.olin.edu/academic-life/student-affairs-resources/student-life/honor-code/">Olin Honor Code</a></b>.</li>

          <li><b>Chat server:</b> Announcements and discussions take place on <a href="https://chat.rpucella.net" target="_blank">our chat server</a>, which double as our homework submission server. You should have received account information by email. If not, please reach out.</li>

  </section>

  <section id="lectures">

    <h2>
      Lectures and Readings
    </h2>

    <p>Subject to changes.</p>

    <ul class="lectures">

      <li><p><span class="hdr">Sep 11: </span><b>Introduction</b></p>
        <div class="lect">
	   <p><a href="https://en.wikipedia.org/wiki/Hilbert's_problems">Hilbert's problems</a>, ten of
	     which were presented at the ICM in Paris in
	     1900. His <a href="https://en.wikipedia.org/wiki/Hilbert's_tenth_problem">tenth
	       problem</a>, on the solvability of Diphantine equations, spoke directly to the notion of
	     computation.</p>
           
      	   <p>The <a href="./demo.hs">Haskell demo</a> that I worked through in class.</p>

           <p>Some notes on <a href="recursion-haskell.txt">writing recursive functions in Haskell</a>.</p>
        </div>
      </li>

      <li><p><b>MACHINE MODELS</b></p></li>

      <li><p><span class="hdr">Sep 18: </span><b>Formal languages</b></p>
        <div class="lect">
           <p>Reading: Chen, pages 8 - 10.</p>
           
           <p>The mathematical notions we'll be using are in Chen
           above. Here's <a href="languages.pdf">additional notes</a> on what I presented in class.
           
	   <p>When talking about set comprehension, I mentioned that there usually needs to be some
	     restrictions on the properties allowed in order to ensure that the sets can be constructed
	     by comprehension. <a href="https://plato.stanford.edu/entries/russell-paradox/">Russell's
	       Paradox</a> is why we need such restrictions.</p>
           
	   <p>You may have noticed that union, intersection, and complementation of sets were defined
	     using set comprehensions that uses conjunction, disjunction, and negation,
	     respectively. This close connection between set operations and logic is a reflection of the
	     fact that sets form a <a href="https://en.wikipedia.org/wiki/Boolean_algebra">Boolean
	       algebra</a>.</p>
           
	   <p>Regular expressions are used to search for patterns in text. <a href="http://www.cs.columbia.edu/~tal/3261/fall07/handout/egrep_mini-tutorial.htm">Here is an example of how they are used in egrep</a>. Note that the equivalence between regular expressions and regular languages only holds for <i>pure</i> regular expressions. Most regular expression packages used in practice extend regular expressions in ways that let you express non-regular languages.</p>
           
           <p>An <a href="https://www.cs.princeton.edu/courses/archive/spr09/cos333/beautiful.html">interesting article by Brian Kernighan</a> on elegant code for regular expression matching. Note that Kernighan's definition of regular expressions is more restricted than ours. (The point of the article is not regular expressions, but elegant code.)</p>
           
	    <p>If you define two regular expressions to be equal when they denote the same set of strings, then regular expressions obey the laws of <a href="https://en.wikipedia.org/wiki/Kleene_algebra">Kleene algebras</a>.</p>
        </div>
      </li>

      <li><p><span class="hdr">Sep 25 – Oct 2: </span><b>Finite state machines</b></p>
        <div class="lect">
           <p>Reading: Chen, Sections 1.1 - 1.4.</p>
           
           <p>Some <a href="automata.pdf">notes</a> on what I presented in class. The transformation
           from regular expressions to finite state machines doesn't go through &epsilon;-NFAs, but
           directly to NFAs, which is a bit more complicated.</p>
           
           <p>One reason to study nondeterministic finite automata is that they can be easier to
           describe than deterministic finite automata. More specifically, there exists languages
           that can be accepted by nondeterministic finite automata with N states whose smallest
           deterministic finite automata that accept those same languages have at least
           2<sup>N</sup> states. One example is the language over {<tt>a</tt>,<tt>b</tt>} consisting
           of all strings whose Nth from last symbol is <tt>a</tt>. <a
           href="https://commons.wikimedia.org/wiki/File:NFA_with_exponential_blown-up_DFA.gif">Here
           is an illustration</a> of the subset construction for that language, over alphabet {0,1},
           and N=4.</p> 

        </div>
      </li>
      
      <li><p><span class="hdr">Oct 11 – 16: </span><b>Turing machines</b></p>
        <div class="lect">
           <p>Reading: Chen, Section 2.1, 2.6, 2.7</p>
           
          <p>A <a href="./tm-anbn.png">diagram</a> of the Turing machine I showed in class to accept the langage {<tt>a</tt><sup>n</sup><tt>b</tt><sup>n</sup> | n &ge; 0}. If you want to practice, you can try to modify the machine to accept the languages {<tt>a</tt><sup>m</sup><tt>b</tt><sup>n</sup> | n &ge; m &ge; 0} and {<tt>a</tt><sup>n</sup><tt>b</tt><sup>n</sup><tt>c</tt><sup>n</sup> | n &ge; 0}.</p>
            
	  <p><a href="https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf"><i>On Computable
	        Numbers, with an Application to the Entscheidungsproblem</i></a>, the original paper by
	    Turing that describes his machines. (Though it's not the most accessible
	    description.)</p>
	  
	  <p>As I mentioned, we can create a variant of finite automata that do not use a tape but that
	    can count. The resulting model is called
	    a <a href="https://en.wikipedia.org/wiki/Counter_automaton">counter automata</a>. When
	    equipped with a single counter, it can accept languages such as
	    {<tt>a</tt><sup>n</sup><tt>b</tt><sup>n</sup> | n &ge; 0}, but cannot accept all
	    computable languages. When equipped with two counters, the resulting machines can accept
	    all computable languages, and are equivalent to Turing machines.</p>

	   <p>The <a href="https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis">Church-Turing
	       thesis</a>.</p>
           
           <p>Pithy mathematical description
             of <a href="https://en.wikipedia.org/wiki/Multitape_Turing_machine">multi-tape Turing
               machines</a>. </p>
           
           <p><a href="register.pdf">Slides</a> describing the simple register machine I
             introduced. It is roughly based on Lambek's abacus model, described in the first chapter
             of <a href="http://www.math.mcgill.ca/barr/papers/pga.pdf"><i>Programs, Grammars,
                 Arguments</i></a>. </p>
           <p> 
        </div>            
      </li>

      <li><p><span class="hdr">Oct 23</span><b>Non-computable languages</b></p>
        <div class="lect">
          <p>Quiz: finite state machines</p>
          
          <p>Reading: Chen, Sections 2.2 &ndash; 2.4</p>

            <p>Wikipedia links: the <a href="https://en.wikipedia.org/wiki/Halting_problem">Halting Problem</a>; <a href="https://en.wikipedia.org/wiki/Reduction_(complexity)">Reducibility</a>; <a href="https://en.wikipedia.org/wiki/Rice%27s_theorem">Rice's Theorem</a>.</p>

            <p>I mentioned that the non-computability of the halting problem for Turing machines impacts any kind of reasoning that we wish to make about programs in modern programming languages. This is a simple application of the Church-Turing thesis. Matthew Might has an accessible account that <a href="https://matt.might.net/articles/intro-static-analysis/">describes how the non-computability of the halting problem impacts static analysis of programs</a>.

            <p>The <a href="https://en.wikipedia.org/wiki/Post_correspondence_problem">Post Correspondence Problem</a> is non-computable, doesn't talk about Turing machines or interpreters for programming languages, and shows that there are reasonable problems that are non-computable. Here's a <a href="pcp-proof.pdf">proof of non-computability</a> from Sipser, basically showing that you can reduce the Halting Problem for Turing machines to PCP.</p>

        </div>
      </li>

      
      <li><p><b>ALTERNATIVE MODELS</b></p></li>

      <li><p><span class="hdr">Oct 30: </span><b>Production grammars</b></p>
      <div class="lect">
            <p>Succinct <a href="./grammars.pdf">notes</a> from the lecture. It includes the
            construction of the unrestricted grammar that can simulate a given Turing machine, which I skipped in class. (These 
            notes are using a slightly different definition of a Turing machine with a marker
            at the beginning of the tape. It is completely equivalent to our definition of Turing
            machines, by a straightforward application of the Church-Turing thesis.)</p>

	    <p>Grammars are used in linguistics (introduced by <a href="https://en.wikipedia.org/wiki/Syntactic_Structures">Chomsky to study the structure of natural languages</a>) and in computer science to implement parsers, that is, turning a string of symbols into a structured artifact like an Abstract Syntax Tree to represent a program in a form that it more amenable to execution and/or compilation. <a href="http://homepages.cwi.nl/~storm/teaching/sc1112/intro-parsing.pdf">Here's
	        a reasonable high-level presentation</a> of the basics of parsing, to give you a flavor.</p>

	    <p>A beautiful application of grammars
	      is <a href="https://en.wikipedia.org/wiki/L-system">L-systems</a>,
	      a class of grammars that model the growth processes of plant
	      development.</p>

	    <p><a href="https://en.wikipedia.org/wiki/Pushdown_automaton">Pushdown automata</a> are machine models that exactly capture context-free languages:
              a language A is context-free if and only if you can find a pushdown automaton that accepts A.</p>
	  </div>
          </div>
      </li>

      <li><p><span class="hdr">Nov 6: </span><b>Lambda calculus (1)</b></p>
      <div class="lect">
          <p>Quiz: Turing machines</p>
            <p><a href="notes/lambda1.html">Notes from the lecture</a>.</p>
	    <p>The fact that the order in which you perform simplifications in the lambda calculus doesn't matter (aside from the possibility of never reaching a normal form) is the <a href="https://en.wikipedia.org/wiki/Church%E2%80%93Rosser_theorem">Church-Rosser property</a>.</p>
	    <p>Some additional introductory references on the lambda calculus: <a href="http://palmstroem.blogspot.com/2012/05/lambda-calculus-for-absolute-dummies.html">The Lambda Calculus for Absolute Dummies (Like Myself)</a> and Rojas's <a href="http://www.inf.fu-berlin.de/lehre/WS03/alpi/lambda.pdf">A Tutorial Introduction to the Lambda Calculus</a>. Note that these references use the standard presentation of the calculus, with <i>&lambda;x.M</i> instead of <i>( \x &rarr; M)</i>. These also cover some of the material I will talk about next time.</p>
          </div>
          
      </div>
      </li>

      <li><p><span class="hdr">Nov 13: </span><b>Lambda calculus (2)</b></p>
      <div class="lect">
          <p>Quiz: register machines</p>
            <p><a href="notes/lambda2.html">Notes from the lecture</a>.</p>

	    <p>A general discussion of <a href="https://en.wikipedia.org/wiki/Fixed-point_combinator">fixed-point combinators</a>, of which &Theta; is an example.</p>

            <p>I showed in class that many of the encodings from the lambda calculus can be directly represented in Haskell. That's because Haskell is in large part based on the lambda calculus. Another language based on the lambda calculus is Lisp, and I would be remiss if I did not point you to <a href="https://sarabander.github.io/sicp/html/index.xhtml">Structure and Interpretation of Computer Programs</a>, which uses a Lisp-like language to explore all sorts of ideas for how to approach the art of programming.</p>
          
      </div>
      </li>

      <li><p><span class="hdr">Nov 27: </span><b>Streams</b></p>
      <div class="lect">
      </div>
      </li>

      <li><p><span class="hdr">Dec 4: </span><b>TBD</b></p>
      <div class="lect">
          <p>Quiz: context-free grammars</p>
      </div>
      </li>

    </ul>

  </section>


  <section id="homeworks">
    
    <h2>Homeworks</h2>
    
    <ul class="plain">
      <li><a href="./homeworks/1">Homework 1</a> &mdash; due Thursday Sep 21</li>
      <li><a href="./homeworks/2">Homework 2</a> &mdash; due Thursday Oct 5</li>
      <li><a href="./homeworks/3">Homework 3</a> &mdash; due Sunday Oct 15</li>
      <li><a href="./homeworks/4">Homework 4</a> &mdash; due Sunday Oct 22</li>
      <li><a href="./homeworks/5">Homework 5</a> &mdash; due Sunday Oct 29</li>
      <li><a href="./homeworks/6">Homework 6</a> &mdash; due Sunday Nov 12</li>
      <li><a href="./homeworks/7">Homework 7</a> &mdash; due Thursday Nov 30</li>
      <li><i>Homework 8 &mdash; due Thursday Dec 7</i></li>
      <li><a href="./project">Final Project</a> &mdash; due Sunday Dec 17</li>
    </ul>
    
  </section>
  
</main>

    
  </body>
</html>
